{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "766fdc19-3e34-4078-a8dd-c8dd43da490b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating the Catalog & Schena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca96885c-ab46-4ba0-bcc0-905f1ef2ccda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG if not exists logistics_catalog_assign;\n",
    "CREATE SCHEMA IF NOT EXISTS logistics_catalog_assign.landing_zone;\n",
    "CREATE VOLUME IF NOT EXISTS logistics_catalog_assign.landing_zone.landing_vol;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22d37948-607e-42ff-81e8-111be51c887f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating the Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e642fc2-83e4-45e7-b4ff-17825ec587f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/Volumes/logistics_catalog_assign/landing_zone/landing_vol/logistics_source1/\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/logistics_catalog_assign/landing_zone/landing_vol/logistics_source2/\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/logistics_catalog_assign/landing_zone/landing_vol/logistics_shipment_detail/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "939466ea-6aec-4e25-b25c-07ce2aaeaa51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Programatically try to find couple of data patterns applying below EDA (File: logistics_source1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af63273b-f0ec-4aee-aa5a-2e806497b80e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Apply inferSchema and toDF to create a DF and analyse the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bc22380-1c84-44e3-9b6b-5852333d00e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "log_src1_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/Volumes/logistics_catalog_assign/landing_zone/landing_vol/logistics_source1/logistics_source1\")\n",
    "\n",
    "log_src1_df.show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e39feb3c-7d10-4bff-b5a6-2681002ed398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Analyse the schema, datatypes, columns etc.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2fc5343-ba00-4960-b55a-9674aafc02b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "log_src1_df.printSchema()\n",
    "print(log_src1_df.schema)\n",
    "print(log_src1_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60d87731-22d6-4884-8a33-033550f08137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Analyse the duplicate records count and summary of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1d60c9f-55d2-46ef-8e8a-54eee00fce18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(log_src1_df.count())\n",
    "print(log_src1_df.distinct().count())\n",
    "display(log_src1_df.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67ec11a1-1153-475b-aae0-27322f96944e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- ###  a. Passive Data Munging - (File: logistics_source1 and logistics_source2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15434d23-cc18-4dc6-8f32-d53f1dd1a28a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "log_src2_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/Volumes/logistics_catalog_assign/landing_zone/landing_vol/logistics_source2/logistics_source2\")\n",
    "display(log_src2_df)\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#shipment_id is non-numeric\n",
    "log_src1_df.filter(~col(\"shipment_id\").rlike(\"^[0-9]+$\")).select(\"shipment_id\").show(truncate=False)\n",
    "log_src2_df.filter(~col(\"shipment_id\").rlike(\"^[0-9]+$\")).select(\"shipment_id\").show(truncate=False)\n",
    "\n",
    "#age is not an integer\n",
    "log_src1_df.filter(~col(\"age\").rlike(\"^[0-9]+$\")).select(\"age\").show(truncate=False)\n",
    "log_src2_df.filter(~col(\"age\").rlike(\"^[0-9]+$\")).select(\"age\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f1ab120-9a80-4ee9-b9a1-c772ed50c1fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### b. Active Data Munging File: logistics_source1 and logistics_source2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab4ca3de-aaa8-4e96-a4ad-9fe22ddc4990",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Read both files without enforcing schema\n",
    "- Align them into a single canonical schema: shipment_id, first_name, last_name, age, role, hub_location, vehicle_type, data_source\n",
    "- Add data_source column with values as: system1, system2 in the respective dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8282d196-4151-422f-92b8-53075a369866",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import lit\n",
    "schema =StructType([StructField('shipment_id', IntegerType(), True),\n",
    "                     StructField('first_name', StringType(), True), \n",
    "                     StructField('last_name', StringType(), True), \n",
    "                     StructField('age', IntegerType(), True),\n",
    "                      StructField('role', StringType(), True), \n",
    "                      StructField('hub_location', StringType(), True), \n",
    "                      StructField('vehicle_type', StringType(), True),\n",
    "                      StructField('data_source', StringType(), True),\n",
    "                      StructField('corruptedrows', StringType(), True)\n",
    "                      ])\n",
    "\n",
    "source1_df=spark.read.schema(schema).csv(path=\"/Volumes/logistics_catalog_assign/landing_zone/landing_vol/logistics_source1/logistics_source1\",mode='permissive',columnNameOfCorruptRecord=\"corruptedrows\",header=True)\n",
    "source1_df=source1_df.withColumn(\"data_source\",lit(\"system1\"))\n",
    "source2_df=spark.read.schema(schema).csv(path=\"/Volumes/logistics_catalog_assign/landing_zone/landing_vol/logistics_source2/logistics_source2\",columnNameOfCorruptRecord=\"corruptedrows\",header=True)\n",
    "source2_df=source2_df.withColumn(\"data_source\",lit(\"system2\"))\n",
    "\n",
    "single_canonical_df = source1_df.union(source2_df)\n",
    "single_canonical_df= single_canonical_df.select(col('shipment_id'),col('first_name'),col('last_name'),col('age'),col('role'),col('hub_location'),col('vehicle_type'),col('data_source'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3158d06c-d4b5-4e70-b306-29cab7077628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Cleansing, Scrubbing:\n",
    "#####Cleansing (removal of unwanted datasets)\n",
    "\n",
    "- Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role\n",
    "- Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name\n",
    "- Join Readiness Rule - Drop records where the join key is null: shipment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66b8d2ad-d9ce-4647-ab93-2bd5eeb77316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(single_canonical_df.count())\n",
    "cleanseddf=single_canonical_df.na.drop(how=\"any\",subset=[\"shipment_id\",\"role\"])\n",
    "print(cleanseddf.count())\n",
    "display(cleanseddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e7ac187-c321-4f1d-8b84-c0778c57835e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf2=cleanseddf.na.drop(how=\"all\",subset=[\"first_name\",\"last_name\"])\n",
    "print(cleanseddf2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2f3275d-f57d-4e1d-bcc3-c16cf90fb5f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf3=cleanseddf2.na.drop(how='all',subset=[\"shipment_id\"])\n",
    "print(cleanseddf2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "375362d9-59a3-4d48-89d2-247b5436966a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Scrubbing (convert raw to tidy)\n",
    "- Age Defaulting Rule - Fill NULL values in the age column with: -1\n",
    "- Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN\n",
    "- Invalid Age Replacement - Replace the following values in age: \"ten\" to -1 \"\" to -1\n",
    "- Vehicle Type Normalization - Replace inconsistent vehicle types: truck to LMV bike to TwoWheeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a545b2ff-5eb4-4ad9-bda9-2841623953e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf4=cleanseddf3.na.fill(-1,['age'])\n",
    "cleanseddf4.where('age=-1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f19d11a-8360-4e24-aa7e-05b1a1a892df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf5=cleanseddf4.na.fill('UNKNOWN',['vehicle_type'])\n",
    "cleanseddf5.where('vehicle_type==\"UNKNOWN\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70a2ab51-7400-4414-85b8-29b53d1d7f02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "find_and_replace = {'Truck':'LMV','Bike':'TwoWheeler'}\n",
    "cleanseddf6=cleanseddf5.na.replace(find_and_replace,subset=['vehicle_type'])\n",
    "cleanseddf6.where('vehicle_type==\"LMV\"').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44fae9db-79fc-4068-85a8-0db0d33893c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 3. Standardization, De-Duplication and Replacement / Deletion of Data to make it in a usable format\n",
    "\n",
    "Creating shipments Details data Dataframe creation\n",
    "\n",
    "Create a DF by Reading Data from logistics_shipment_detail.json\n",
    "As this data is a clean json data, it doesn't require any cleansing or scrubbing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b8e470-90c6-4caf-ada2-bed95a35533f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 25"
    }
   },
   "outputs": [],
   "source": [
    "logistics_shipment_df = (\n",
    "    spark.read\n",
    "         .option(\"multiline\", \"true\")   \n",
    "         .option(\"mode\", \"PERMISSIVE\") \n",
    "         .json(\"/Volumes/logistics_catalog_assign/landing_zone/landing_vol/logistics_shipment_detail/logistics_shipment_detail_3000.json\")\n",
    ")\n",
    "\n",
    "logistics_shipment_df.show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ba7ca54-4c4d-44bb-a523-96a4a29edd32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Add a column\n",
    "Source File: DF of logistics_shipment_detail_3000.json\n",
    "- domain as 'Logistics', current timestamp 'ingestion_timestamp' and 'False' as 'is_expedited'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ed74899-f4cb-4ac3-bf81-6557d9e03f51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp,lit\n",
    "logistics_shipment_df2 = logistics_shipment_df.withColumn(\"domain\",lit(\"Logistics\")).withColumn(\"ingestion_timestamp\",current_timestamp()).withColumn(\"is_expedited\",lit(\"False\"))\n",
    "logistics_shipment_df2.show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa948f4d-0fdf-4252-8ec7-a3267dca149f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Column Uniformity: role - Convert to lowercase\n",
    "- Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "  - vehicle_type - Convert values to UPPERCASE\n",
    "- Source Files: DF of logistics_shipment_detail_3000.json             \n",
    "  - hub_location - Convert values to initcap case\n",
    "- Source Files: DF of merged(logistics_source1 & logistics_source2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cd403e9-d2f4-466c-9592-5dda3129af89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper,initcap\n",
    "cleanseddf7=cleanseddf6.withColumn(\"vehicle_type\",upper(col(\"vehicle_type\"))).withColumn(\"hub_location\",initcap(col(\"hub_location\")))\n",
    "cleanseddf7.where('data_source == \"system2\"').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80945e27-bcf9-4b12-8bc7-98434c4010f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Format Standardization:\n",
    "- Source Files: DF of logistics_shipment_detail_3000.json\n",
    "  - Convert shipment_date to yyyy-MM-dd\n",
    "  - Ensure shipment_cost has 2 decimal precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d865fa6-d641-4bbe-ba83-aa0bbd2eb6ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date,date_format\n",
    "logistics_shipment_df3 = logistics_shipment_df2.withColumn(\"shipment_date\", date_format(to_date(col(\"shipment_date\"), \"yy-MM-dd\"),\"MM-dd-yyyy\")).withColumn(\"shipment_cost\", col(\"shipment_cost\").cast(\"decimal(18,2)\"))\n",
    "logistics_shipment_df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "565ada3a-bc0f-4c89-8296-28ce00c6f9f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data Type Standardization\n",
    "Standardizing column data types to fix schema drift and enable mathematical operations.\n",
    " - Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "   - age: Cast String to Integer\n",
    " - Source File: DF of logistics_shipment_detail_3000.json\n",
    "   - shipment_weight_kg: Cast to Double\n",
    "- Source File: DF of logistics_shipment_detail_3000.json\n",
    "   - is_expedited: Cast to Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea04cb60-1654-4441-b799-e1947c5bf353",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf8 = cleanseddf7.withColumn(\"age\",col('age').cast(\"int\"))\n",
    "logistics_shipment_df4 = logistics_shipment_df3.withColumn(\"shipment_weight\",col('shipment_weight_kg').cast(\"double\")).withColumn(\"is_expedited\",col('is_expedited').cast(\"boolean\"))\n",
    "logistics_shipment_df4.show(10000,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a58e6ded-7f3d-4e5b-aae8-8dc9a7a38ce5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Naming Standardization\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    " - Rename: first_name to staff_first_name\n",
    " - Rename: last_name to staff_last_name\n",
    " - Rename: hub_location to origin_hub_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b3996e4-2d40-4e5d-8663-edd839ded8a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf9 = cleanseddf8.withColumnRenamed(\"first_name\",\"staff_first_name\").withColumnRenamed(\"last_name\",\"staff_last_name\").withColumnRenamed(\"hub_location\",\"origin_hub_city\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c01994-813c-4a33-a888-2245199f1b12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reordering columns logically in a better standard format:\n",
    "Source File: DF of Data from all 3 files\n",
    "shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "815eb1fe-d913-46c3-a600-e57340f68db2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf10 = cleanseddf9.selectExpr('shipment_id','staff_first_name','staff_last_name','age','role','origin_hub_city','vehicle_type','data_source')\n",
    "cleanseddf10.show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd91d9b1-41fc-4fd3-afed-3b7be95fb16b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logistics_shipment_df5 = logistics_shipment_df4.selectExpr('shipment_id','domain','cargo_type','source_city','destination_city','order_id','shipment_date','shipment_cost','shipment_weight','shipment_status','is_expedited','ingestion_timestamp')\n",
    "logistics_shipment_df5.show(100,truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a79c6d30-a73c-4998-a093-070ff43e5004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Deduplication:\n",
    "\n",
    " - Apply Record Level De-Duplication\n",
    " - Apply Column Level De-Duplication (Primary Key Enforcement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "688ef029-5dc9-471c-9597-95f4913a76a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf11 = cleanseddf10.dropDuplicates()\n",
    "cleanseddf12 = cleanseddf11.dropDuplicates(['shipment_id'])\n",
    "display(cleanseddf12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ca11c1e-5929-4454-bc01-56113d8774a2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768593534803}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logistics_shipment_df6 = logistics_shipment_df5.dropDuplicates()\n",
    "logistics_shipment_df7 = logistics_shipment_df6.dropDuplicates(['shipment_id'])\n",
    "display(logistics_shipment_df7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fb7fdd2-804c-402c-b254-dc9d736333db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Data Enrichment - Detailing of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cea6f3c8-72f7-4c15-91b8-6bd5847b93bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Add Audit Timestamp (load_dt) Source File: DF of logistics_source1 and logistics_source2\n",
    "\n",
    " - Scenario: We need to track exactly when this record was ingested into our Data Lakehouse for auditing purposes.\n",
    " - Action: Add a column load_dt using the function current_timestamp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc722af1-b3ed-484b-bb0a-e1031a702081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf13 = cleanseddf12.withColumn(\"load_dt\",current_timestamp())\n",
    "cleanseddf13.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d57f57c5-4622-4f13-b894-380733dc8d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Create Full Name (full_name) Source File: DF of logistics_source1 and logistics_source2\n",
    "\n",
    " - Scenario: The reporting dashboard requires a single field for the driver's name instead of separate columns.\n",
    " - Action: Create full_name by concatenating first_name and last_name with a space separator.\n",
    "Result: \"Rajesh\" + \" \" + \"Kumar\" -> \"Rajesh Kumar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a6dc4e6-a0ba-4a1e-a3ea-40ea3d2acee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "cleanseddf14 = cleanseddf13.withColumn(\"full_name\",concat(col(\"staff_first_name\"),lit(\" \"),col(\"staff_last_name\")))\n",
    "display(cleanseddf14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6c8317f-5081-4a5a-89f8-4802f64e1471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Define Route Segment (route_segment) Source File: DF of logistics_shipment_detail_3000.json\n",
    "\n",
    " - Scenario: The logistics team wants to analyze performance based on specific transport lanes (Source to Destination).\n",
    " - Action: Combine source_city and destination_city with a hyphen.\n",
    " - Result: \"Chennai\" + \"-\" + \"Pune\" -> \"Chennai-Pune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f06ddb4e-a806-43f4-a86a-a739a9b92c5f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768593623696}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logistics_shipment_df8 = logistics_shipment_df7.withColumn(\"route_segment\", concat(col(\"source_city\"),lit(\"->\"),col(\"destination_city\")))\n",
    "display(logistics_shipment_df8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a353c9a4-5a0e-41d3-9629-a930328fbc3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Generate Vehicle Identifier (vehicle_identifier) Source File: DF of logistics_shipment_detail_3000.json\n",
    "\n",
    " - Scenario: We need a unique tracking code that immediately tells us the vehicle type and the shipment ID.\n",
    " - Action: Combine vehicle_type and shipment_id to create a composite key.\n",
    " - Result: \"Truck\" + \"_\" + \"500001\" -> \"Truck_500001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0ff2054-d479-49b9-812f-8f9d057da24e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf15 = cleanseddf14.withColumn(\"vehicle_identifier\",concat(col('vehicle_type'),lit(\"_\"),col('shipment_id')))\n",
    "display(cleanseddf15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d4c6be1-657b-48d7-ad87-2138d4ea3994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Derive Shipment Year (shipment_year)\n",
    "\n",
    " - Scenario: Management needs an annual performance report to compare growth year-over-year.\n",
    " - Action: Extract the year component from shipment_date.-\n",
    " - Result: \"2024-04-23\" -> 2024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6ab1dbb-09cb-49d3-89dd-8e283ba89d14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logistics_shipment_df9 = logistics_shipment_df8.withColumn(\"shipment_year\",year(to_date(col('shipment_date'),'MM-dd-yyyy')))\n",
    "logistics_shipment_df9.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b236d1a8-4d76-4791-b977-1df4649ba70c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Derive Shipment Month (shipment_month)\n",
    "\n",
    " - Scenario: Analysts want to identify seasonal peaks (e.g., increased volume in December).\n",
    " - Action: Extract the month component from shipment_date.\n",
    " - Result: \"2024-04-23\" -> 4 (April)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1921f427-ad8f-4502-8369-08ab976d89e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logistics_shipment_df10 = logistics_shipment_df9.withColumn(\"shipment_month\",month(to_date(col('shipment_date'),'MM-dd-yyyy')))\n",
    "logistics_shipment_df10.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0845532d-d0e4-4f8e-8c3e-262f8034b5bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Flag shipment status (is_expedited)\n",
    "\n",
    "- Scenario: The Operations team needs to track shipments is IN_TRANSIT or DELIVERED.\n",
    "-  Action: Flag as 'True' if the shipment_status IN_TRANSIT or DELIVERED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9169c1ee-02f8-461e-a420-de891f3d081e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logistics_shipment_df11 =logistics_shipment_df10.withColumn(\"is_expedited\",when(col(\"shipment_status\")==\"IN_TRANSIT\",True).when(col(\"shipment_status\")==\"DELIVERED\",True).otherwise(col(\"is_expedited\")))\n",
    "logistics_shipment_df11.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f71fc31-d0c7-45b2-9a8a-3bde59a5dcc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Enrichment/Business Logics (Calculated Fields)**\n",
    " - Deriving new metrics and financial indicators using mathematical and date-based operations.\n",
    " - Source File: logistics_shipment_detail_3000.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8316ee2f-a2f3-4275-8f7b-bed593711cc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " 1. Calculate Unit Cost (cost_per_kg)\n",
    "\n",
    "- Scenario: The Finance team wants to analyze the efficiency of shipments by determining the cost incurred per unit of weight.\n",
    " - Action: Divide shipment_cost by shipment_weight_kg.\n",
    " - Logic: shipment_cost / shipment_weight_kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5956d84-99d2-47ca-87d1-79df0e4d3d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logistics_shipment_df12 = logistics_shipment_df11.withColumn(\"cost_per_kg\",col(\"shipment_cost\")/col(\"shipment_weight\")).withColumn(\"cost_per_kg\",col(\"cost_per_kg\").cast(\"decimal(18,2)\"))\n",
    "logistics_shipment_df12.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8298865f-98c9-47ae-b2fd-adf8176a6301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Track Shipment Age (days_since_shipment)\n",
    "\n",
    " - Scenario: The Operations team needs to monitor how long it has been since a shipment was dispatched to identify potential delays.\n",
    " - Action: Calculate the difference in days between the current_date and the shipment_date.\n",
    " - Logic: datediff(current_date(), shipment_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b8905a-3ddd-42d2-9e65-2d0698a94f8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logistics_shipment_df13 =logistics_shipment_df12.withColumn(\"days_since_shipment\",datediff(current_date(),to_date(col('shipment_date'),'MM-dd-yyyy')))\n",
    "logistics_shipment_df13.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "089a9b82-a845-4174-ae0b-b6370124ad43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Compute Tax Liability (tax_amount)\n",
    "\n",
    " - Scenario: For invoicing and compliance, we must calculate the Goods and Services Tax (GST) applicable to each shipment.\n",
    " - Action: Calculate 18% GST on the total shipment_cost.\n",
    " - Logic: shipment_cost * 0.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f244d6ce-2a61-4aff-8055-010a074c897f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logistics_shipment_df14 = logistics_shipment_df13.withColumn(\"tax_amount\",col(\"shipment_cost\")*0.18).withColumn(\"tax_amount\",col(\"tax_amount\").cast(\"decimal(18,2)\"))\n",
    "logistics_shipment_df14.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c57e7f9e-4a6f-41f9-9199-52ebb4cfb04a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Remove/Eliminate (drop, select, selectExpr)**\n",
    " - Excluding unnecessary or redundant columns to optimize storage and privacy.\n",
    " - Source File: DF of logistics_source1 and logistics_source2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0b33074-882a-4fb7-bf12-f1efaeb1d43c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Remove Redundant Name Columns\n",
    "\n",
    " - Scenario: Since we have already created the full_name column in the Enrichment step, the individual name columns are now redundant and clutter the dataset.\n",
    "- Action: Drop the first_name and last_name columns.\n",
    " - Logic: df.drop(\"first_name\", \"last_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89a5db8a-11b1-4fa5-9723-68521595cccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf16 = cleanseddf15.drop(\"staff_first_name\",\"staff_last_name\")\n",
    "display(cleanseddf16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3352781-07b5-4f71-a489-f16c1336f6fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Splitting & Merging/Melting of Columns**                            \n",
    "Reshaping columns to extract hidden values or combine fields for better analysis.\n",
    "Source File: DF of logistics_shipment_detail_3000.json\n",
    "1. Splitting (Extraction) Breaking one column into multiple to isolate key information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "713f8898-b6fa-4290-9e99-58b96308a1f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Split Order Code:\n",
    " - Action: Split order_id (\"ORD100000\") into two new columns:\n",
    "order_prefix (\"ORD\")\n",
    "order_sequence (\"100000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7beec83-54cc-4091-a362-21362f916333",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768630312416}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logistics_shipment_df15 = logistics_shipment_df14.withColumn(\"order_prefix\",substring(col(\"order_id\"),0,3)).withColumn(\"order_suffix\",substring(col(\"order_id\"),4,length(col(\"order_id\"))))\n",
    "display(logistics_shipment_df15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa9f73e7-ac5c-4cac-b13e-0e0eeeba0cfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Split Date:\n",
    " - Action: Split shipment_date into three separate columns for partitioning:\n",
    "  - ship_year (2024)\n",
    "  - ship_month (4)\n",
    "  - ship_day (23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "966db88c-c561-402d-994b-05aa375e5634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logistics_shipment_df16 = logistics_shipment_df15.withColumn(\"shipment_month\",day(to_date(col('shipment_date'),'MM-dd-yyyy')))\n",
    "logistics_shipment_df16.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a5fbd03-31ca-4011-8466-974a224b6907",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3. Data Customization & Processing - Application of Tailored Business Specific Rules**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a24d593-8128-428c-8818-a49b457dd0bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**UDF1: Complex Incentive Calculation**                        \n",
    " - **Scenario**: The Logistics Head wants to calculate a \"Performance Bonus\" for drivers based on tenure and role complexity.\n",
    "\n",
    " - **Action**: Create a Python function calculate_bonus(role, age) and register it as a Spark UDF.\n",
    "\n",
    "- **Logic**:\n",
    "\n",
    "  - IF Role == 'Driver' AND Age > 50: Bonus = 15% of Salary (Reward for Seniority)        \n",
    "  - IF Role == 'Driver' AND Age < 30: Bonus = 5% of Salary (Encouragement for Juniors)\n",
    "  - ELSE: Bonus = 0       \n",
    " - **Result**: A new derived column projected_bonus is generated for every row in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2491a1b-1026-4e6e-967b-3d3a0646829f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_bonus(role,age):\n",
    "    if role==\"Driver\" and age>50:\n",
    "        return 15\n",
    "    elif role==\"Driver\" and age<30:\n",
    "        return 5\n",
    "    else:\n",
    "        return 0\n",
    "bonusUDF = udf(calculate_bonus)\n",
    "cleanseddf17 = cleanseddf16.withColumn(\"projected_bonus\",bonusUDF(col(\"role\"),col(\"age\"))).withColumn(\"projected_bonus\",col(\"projected_bonus\").cast(\"decimal(18,2)\"))\n",
    "cleanseddf17.where('projected_bonus > 0.0').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2639d43f-acc9-455c-a0db-2cb930ffb7c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**UDF2: PII Masking (Privacy Compliance)**    \n",
    " - **Scenario**: For the analytics dashboard, we must hide the full identity of the staff to comply with privacy laws (GDPR/DPDP), while keeping names recognizable for internal managers.\n",
    "\n",
    " - **Business Rule**: Show the first 2 letters, mask the middle characters with ****, and show the last letter.\n",
    "\n",
    " - **Action**: Create a UDF mask_identity(name).\n",
    "\n",
    "Example:\n",
    "\n",
    "Input: \"Rajesh\"     \n",
    "Output: \"Ra****h\"     \n",
    "Note: Convert the above udf logic to inbult function based transformation to ensure the performance is improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae1be0e2-e0f5-413a-a4bb-155a05834813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def mask_identity(fullname):\n",
    "    if fullname is None:\n",
    "        return None\n",
    "    if len(fullname) == 0:\n",
    "        return None\n",
    "    if len(fullname) < 3:\n",
    "        return fullname[:1] + \"*\" + fullname[-1]\n",
    "    else:\n",
    "        return fullname[:2] + \"**\" + fullname[-1]\n",
    "maskUDF = udf(mask_identity,StringType())\n",
    "cleanseddf18 = cleanseddf17.withColumn(\"masked_name\",maskUDF(split(col(\"full_name\"),' ')[0]))\n",
    "cleanseddf18.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2f86bcf-b77c-48c3-b463-83b0b16cbe9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**4. Data Core Curation & Processing (Pre-Wrangling)**    \n",
    "Applying business logic to focus, filter, and summarize data before final analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e69db3c2-8875-4b08-9742-0db1f97549dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Select** (Projection)\n",
    "Source Files: DF of logistics_source1 and logistics_source2\n",
    "\n",
    "**Scenario**: The Driver App team only needs location data, not sensitive HR info.    \n",
    "**Action**: Select only first_name, role, and hub_location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa3e92f3-09c9-4022-a1c6-f7775207bc64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "driver_app_df = cleanseddf18.selectExpr(\"full_name\",\"role\",\"origin_hub_city\")\n",
    "display(driver_app_df.where('origin_hub_city is not null'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77694a2b-58e7-444a-a115-46e8866dbb35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. **Filter** (Selection)   \n",
    "Source File: DF of json\n",
    "\n",
    "**Scenario**: We need a report on active operational problems.  \n",
    "**Action**: Filter rows where shipment_status is 'DELAYED' or 'RETURNED'.     \n",
    "**Scenario**: Insurance audit for senior staff.   \n",
    "**Action**: Filter rows where age > 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b4a8089-d1bc-41e2-b7b3-d8f5541dbfaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delay_status_df = logistics_shipment_df16.filter(col('shipment_status').isin(\"RETURNED\",\"DELAYED\"))\n",
    "delay_status_df.show()\n",
    "insurance_audit_df = cleanseddf18.filter(col('age')>50)\n",
    "insurance_audit_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44ff0d27-d5ea-4b8e-b155-623d33c07b64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3. Derive Flags & Columns** (Business Logic)      \n",
    "Source File: DF of json\n",
    "\n",
    "**Scenario**: Identify high-value shipments for security tracking.  \n",
    "**Action**: Create flag is_high_value = True if shipment_cost > 40,000.\n",
    "**Scenario**: Flag weekend operations for overtime calculation.   \n",
    "**Action**: Create flag is_weekend = True if day is Saturday or Sunday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77f0783d-e372-49d8-9c6f-74a08547b3ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logistics_shipment_df17 = logistics_shipment_df16.withColumn(\"is_high_value\",when(col(\"shipment_cost\")>40000,True).otherwise(False)).withColumn(\"is_weekend\",when((dayofweek(to_date(col(\"shipment_date\"),\"MM-dd-yyyy\"))==1) | (dayofweek(to_date(col(\"shipment_date\"),\"MM-dd-yyyy\"))==7),True).otherwise(False))\n",
    "logistics_shipment_df17.where('is_high_value==True').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a16dca1a-03f2-4a77-ad9f-c256e6f0b028",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. **Format** (Standardization)   \n",
    "Source File: DF of json\n",
    "\n",
    "**Scenario**: Finance requires readable currency formats.   \n",
    "**Action**: Format shipment_cost to string like \"30,695.80\".   \n",
    "**Scenario**: Standardize city names for reporting.   \n",
    "**Action**: Format source_city to Uppercase (e.g., \"chennai\"  \"CHENNAI\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e1bb082-5d58-4dd5-841a-c2c9b12bfcd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logistics_shipment_df18 = logistics_shipment_df17.withColumn(\"shipment_cost\",concat(lit(\"$\"),format_number(col(\"shipment_cost\"),2))).withColumn(\"source_city\",upper(col(\"source_city\"))).withColumn(\"destination_city\",upper(col(\"destination_city\")))\n",
    "logistics_shipment_df18.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94a09953-3890-4e1a-987e-7777a4b2e407",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5. Group & Aggregate** (Summarization)    \n",
    "Source Files: DF of logistics_source1 and logistics_source2\n",
    "\n",
    "**Scenario**: Regional staffing analysis.   \n",
    "**Action**: Group by hub_location and Count the number of staff.    \n",
    "**Scenario**: Fleet capacity analysis.  \n",
    "**Action**: Group by shimpent_status and Sum the shipment_weight_kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16779b58-1e7e-4a3a-b6ab-e6b74057cfa4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768641828812}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "location_wise_count= cleanseddf18.groupBy(\"origin_hub_city\").agg(countDistinct(\"shipment_id\").alias(\"distinct_shipments\"))\n",
    "display(location_wise_count)\n",
    "vehicle_type_wise_shipment_weight = logistics_shipment_df18.groupBy(\"shipment_status\").agg(sum(\"shipment_weight\").alias(\"total_weight\"))\n",
    "display(vehicle_type_wise_shipment_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1c3cb9d-10d7-4e2b-906d-bda14821f621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**6. Sorting** (Ordering)   \n",
    "Source File: DF of json\n",
    "\n",
    "**Scenario**: Prioritize the most expensive shipments.  \n",
    "**Action**: Sort by shipment_cost in Descending order.    \n",
    "**Scenario**: Organize daily dispatch schedule.   \n",
    "**Action**: Sort by shipment_date (Ascending)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7d8ec43-5535-451b-8db3-7d6c66f19c86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logistics_shipment_df19=logistics_shipment_df18.sort([col(\"shipment_date\"),col('shipment_cost')],ascending=[False,False])\n",
    "logistics_shipment_df19.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "866a963c-94a5-4f37-ba1e-4f53c23b7e7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**7. Limit** (Top-N Analysis)   \n",
    "Source File: DF of json\n",
    "\n",
    "**Scenario**: Dashboard snapshot of critical delays.  \n",
    "**Action**: Filter for 'DELAYED', Sort by Cost, and Limit to top 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07b5300e-67f5-4816-9b43-a9ba04e257c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_delay_df = logistics_shipment_df19.filter(col('shipment_status') == 'DELAYED').sort(col(\"shipment_cost\").desc()).limit(10)\n",
    "top_delay_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfa2a95c-a631-4bc1-9391-8eeed23e7176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6089303672176865,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Logistics UseCase",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
