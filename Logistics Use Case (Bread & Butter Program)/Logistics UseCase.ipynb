{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "766fdc19-3e34-4078-a8dd-c8dd43da490b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating the Catalog & Schena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca96885c-ab46-4ba0-bcc0-905f1ef2ccda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG if not exists logistics_catalog_assign;\n",
    "CREATE SCHEMA IF NOT EXISTS logistics_catalog_assign.landing_zone;\n",
    "CREATE VOLUME IF NOT EXISTS logistics_catalog_assign.landing_zone.landing_vol;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22d37948-607e-42ff-81e8-111be51c887f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating the Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e642fc2-83e4-45e7-b4ff-17825ec587f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/Volumes/logistics_catalog_assign/landing_zone/landing_vol/logistics_source1/\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/logistics_catalog_assign/landing_zone/landing_vol/logistics_source2/\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/logistics_catalog_assign/landing_zone/landing_vol/logistics_shipment_detail/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "939466ea-6aec-4e25-b25c-07ce2aaeaa51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Programatically try to find couple of data patterns applying below EDA (File: logistics_source1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af63273b-f0ec-4aee-aa5a-2e806497b80e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Apply inferSchema and toDF to create a DF and analyse the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bc22380-1c84-44e3-9b6b-5852333d00e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "log_src1_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/Volumes/logistics_catalog_assign/landing_zone/landing_vol/logistics_source1/logistics_source1\")\n",
    "\n",
    "log_src1_df.show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e39feb3c-7d10-4bff-b5a6-2681002ed398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Analyse the schema, datatypes, columns etc.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2fc5343-ba00-4960-b55a-9674aafc02b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "log_src1_df.printSchema()\n",
    "print(log_src1_df.schema)\n",
    "print(log_src1_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60d87731-22d6-4884-8a33-033550f08137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Analyse the duplicate records count and summary of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1d60c9f-55d2-46ef-8e8a-54eee00fce18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(log_src1_df.count())\n",
    "print(log_src1_df.distinct().count())\n",
    "display(log_src1_df.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67ec11a1-1153-475b-aae0-27322f96944e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- ###  a. Passive Data Munging - (File: logistics_source1 and logistics_source2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15434d23-cc18-4dc6-8f32-d53f1dd1a28a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "log_src2_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/Volumes/logistics_catalog_assign/landing_zone/landing_vol/logistics_source2/logistics_source2\")\n",
    "display(log_src2_df)\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#shipment_id is non-numeric\n",
    "log_src1_df.filter(~col(\"shipment_id\").rlike(\"^[0-9]+$\")).select(\"shipment_id\").show(truncate=False)\n",
    "log_src2_df.filter(~col(\"shipment_id\").rlike(\"^[0-9]+$\")).select(\"shipment_id\").show(truncate=False)\n",
    "\n",
    "#age is not an integer\n",
    "log_src1_df.filter(~col(\"age\").rlike(\"^[0-9]+$\")).select(\"age\").show(truncate=False)\n",
    "log_src2_df.filter(~col(\"age\").rlike(\"^[0-9]+$\")).select(\"age\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f1ab120-9a80-4ee9-b9a1-c772ed50c1fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### b. Active Data Munging File: logistics_source1 and logistics_source2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab4ca3de-aaa8-4e96-a4ad-9fe22ddc4990",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Read both files without enforcing schema\n",
    "- Align them into a single canonical schema: shipment_id, first_name, last_name, age, role, hub_location, vehicle_type, data_source\n",
    "- Add data_source column with values as: system1, system2 in the respective dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8282d196-4151-422f-92b8-53075a369866",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import lit\n",
    "schema =StructType([StructField('shipment_id', IntegerType(), True),\n",
    "                     StructField('first_name', StringType(), True), \n",
    "                     StructField('last_name', StringType(), True), \n",
    "                     StructField('age', IntegerType(), True),\n",
    "                      StructField('role', StringType(), True), \n",
    "                      StructField('hub_location', StringType(), True), \n",
    "                      StructField('vehicle_type', StringType(), True),\n",
    "                      StructField('data_source', StringType(), True),\n",
    "                      StructField('corruptedrows', StringType(), True)\n",
    "                      ])\n",
    "\n",
    "source1_df=spark.read.schema(schema).csv(path=\"/Volumes/logistics_catalog_assign/landing_zone/landing_vol/logistics_source1/logistics_source1\",mode='permissive',columnNameOfCorruptRecord=\"corruptedrows\",header=True)\n",
    "source1_df=source1_df.withColumn(\"data_source\",lit(\"system1\"))\n",
    "source2_df=spark.read.schema(schema).csv(path=\"/Volumes/logistics_catalog_assign/landing_zone/landing_vol/logistics_source2/logistics_source2\",columnNameOfCorruptRecord=\"corruptedrows\",header=True)\n",
    "source2_df=source2_df.withColumn(\"data_source\",lit(\"system2\"))\n",
    "\n",
    "single_canonical_df = source1_df.union(source2_df)\n",
    "single_canonical_df= single_canonical_df.select(col('shipment_id'),col('first_name'),col('last_name'),col('age'),col('role'),col('hub_location'),col('vehicle_type'),col('data_source'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3158d06c-d4b5-4e70-b306-29cab7077628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Cleansing, Scrubbing:\n",
    "#####Cleansing (removal of unwanted datasets)\n",
    "\n",
    "- Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role\n",
    "- Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name\n",
    "- Join Readiness Rule - Drop records where the join key is null: shipment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66b8d2ad-d9ce-4647-ab93-2bd5eeb77316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(single_canonical_df.count())\n",
    "cleanseddf=single_canonical_df.na.drop(how=\"any\",subset=[\"shipment_id\",\"role\"])\n",
    "print(cleanseddf.count())\n",
    "display(cleanseddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e7ac187-c321-4f1d-8b84-c0778c57835e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf2=cleanseddf.na.drop(how=\"all\",subset=[\"first_name\",\"last_name\"])\n",
    "print(cleanseddf2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2f3275d-f57d-4e1d-bcc3-c16cf90fb5f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf3=cleanseddf2.na.drop(how='all',subset=[\"shipment_id\"])\n",
    "print(cleanseddf2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "375362d9-59a3-4d48-89d2-247b5436966a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Scrubbing (convert raw to tidy)\n",
    "- Age Defaulting Rule - Fill NULL values in the age column with: -1\n",
    "- Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN\n",
    "- Invalid Age Replacement - Replace the following values in age: \"ten\" to -1 \"\" to -1\n",
    "- Vehicle Type Normalization - Replace inconsistent vehicle types: truck to LMV bike to TwoWheeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a545b2ff-5eb4-4ad9-bda9-2841623953e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf4=cleanseddf3.na.fill(-1,['age'])\n",
    "cleanseddf4.where('age=-1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f19d11a-8360-4e24-aa7e-05b1a1a892df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf5=cleanseddf4.na.fill('UNKNOWN',['vehicle_type'])\n",
    "cleanseddf5.where('vehicle_type==\"UNKNOWN\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70a2ab51-7400-4414-85b8-29b53d1d7f02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "find_and_replace = {'Truck':'LMV','Bike':'TwoWheeler'}\n",
    "cleanseddf6=cleanseddf5.na.replace(find_and_replace,subset=['vehicle_type'])\n",
    "cleanseddf6.where('vehicle_type==\"LMV\"').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44fae9db-79fc-4068-85a8-0db0d33893c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 3. Standardization, De-Duplication and Replacement / Deletion of Data to make it in a usable format\n",
    "\n",
    "Creating shipments Details data Dataframe creation\n",
    "\n",
    "Create a DF by Reading Data from logistics_shipment_detail.json\n",
    "As this data is a clean json data, it doesn't require any cleansing or scrubbing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b8e470-90c6-4caf-ada2-bed95a35533f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 25"
    }
   },
   "outputs": [],
   "source": [
    "logistics_shipment_df = (\n",
    "    spark.read\n",
    "         .option(\"multiline\", \"true\")   \n",
    "         .option(\"mode\", \"PERMISSIVE\") \n",
    "         .json(\"/Volumes/logistics_catalog_assign/landing_zone/landing_vol/logistics_shipment_detail/logistics_shipment_detail_3000.json\")\n",
    ")\n",
    "\n",
    "logistics_shipment_df.show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ba7ca54-4c4d-44bb-a523-96a4a29edd32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Add a column\n",
    "Source File: DF of logistics_shipment_detail_3000.json\n",
    "- domain as 'Logistics', current timestamp 'ingestion_timestamp' and 'False' as 'is_expedited'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ed74899-f4cb-4ac3-bf81-6557d9e03f51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp,lit\n",
    "logistics_shipment_df2 = logistics_shipment_df.withColumn(\"domain\",lit(\"Logistics\")).withColumn(\"ingestion_timestamp\",current_timestamp()).withColumn(\"is_expedited\",lit(\"False\"))\n",
    "logistics_shipment_df2.show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa948f4d-0fdf-4252-8ec7-a3267dca149f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Column Uniformity: role - Convert to lowercase\n",
    "- Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "  - vehicle_type - Convert values to UPPERCASE\n",
    "- Source Files: DF of logistics_shipment_detail_3000.json             \n",
    "  - hub_location - Convert values to initcap case\n",
    "- Source Files: DF of merged(logistics_source1 & logistics_source2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cd403e9-d2f4-466c-9592-5dda3129af89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper,initcap\n",
    "cleanseddf7=cleanseddf6.withColumn(\"vehicle_type\",upper(col(\"vehicle_type\"))).withColumn(\"hub_location\",initcap(col(\"hub_location\")))\n",
    "cleanseddf7.where('data_source == \"system2\"').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80945e27-bcf9-4b12-8bc7-98434c4010f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Format Standardization:\n",
    "- Source Files: DF of logistics_shipment_detail_3000.json\n",
    "  - Convert shipment_date to yyyy-MM-dd\n",
    "  - Ensure shipment_cost has 2 decimal precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d865fa6-d641-4bbe-ba83-aa0bbd2eb6ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date,date_format\n",
    "logistics_shipment_df3 = logistics_shipment_df2.withColumn(\"shipment_date\", date_format(to_date(col(\"shipment_date\"), \"yy-MM-dd\"),\"MM-dd-yyyy\")).withColumn(\"shipment_cost\", col(\"shipment_cost\").cast(\"decimal(18,2)\"))\n",
    "logistics_shipment_df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "565ada3a-bc0f-4c89-8296-28ce00c6f9f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data Type Standardization\n",
    "Standardizing column data types to fix schema drift and enable mathematical operations.\n",
    " - Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "   - age: Cast String to Integer\n",
    " - Source File: DF of logistics_shipment_detail_3000.json\n",
    "   - shipment_weight_kg: Cast to Double\n",
    "- Source File: DF of logistics_shipment_detail_3000.json\n",
    "   - is_expedited: Cast to Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea04cb60-1654-4441-b799-e1947c5bf353",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf8 = cleanseddf7.withColumn(\"age\",col('age').cast(\"int\"))\n",
    "logistics_shipment_df4 = logistics_shipment_df3.withColumn(\"shipment_weight\",col('shipment_weight_kg').cast(\"double\")).withColumn(\"is_expedited\",col('is_expedited').cast(\"boolean\"))\n",
    "logistics_shipment_df4.show(10000,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a58e6ded-7f3d-4e5b-aae8-8dc9a7a38ce5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Naming Standardization\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    " - Rename: first_name to staff_first_name\n",
    " - Rename: last_name to staff_last_name\n",
    " - Rename: hub_location to origin_hub_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b3996e4-2d40-4e5d-8663-edd839ded8a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf9 = cleanseddf8.withColumnRenamed(\"first_name\",\"staff_first_name\").withColumnRenamed(\"last_name\",\"staff_last_name\").withColumnRenamed(\"hub_location\",\"origin_hub_city\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c01994-813c-4a33-a888-2245199f1b12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reordering columns logically in a better standard format:\n",
    "Source File: DF of Data from all 3 files\n",
    "shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "815eb1fe-d913-46c3-a600-e57340f68db2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf10 = cleanseddf9.selectExpr('shipment_id','staff_first_name','staff_last_name','age','role','origin_hub_city','vehicle_type','data_source')\n",
    "cleanseddf10.show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd91d9b1-41fc-4fd3-afed-3b7be95fb16b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logistics_shipment_df5 = logistics_shipment_df4.selectExpr('shipment_id','domain','cargo_type','source_city','destination_city','order_id','shipment_date','shipment_cost','shipment_weight','is_expedited','ingestion_timestamp')\n",
    "logistics_shipment_df5.show(100,truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a79c6d30-a73c-4998-a093-070ff43e5004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Deduplication:\n",
    "\n",
    " - Apply Record Level De-Duplication\n",
    " - Apply Column Level De-Duplication (Primary Key Enforcement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "688ef029-5dc9-471c-9597-95f4913a76a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf11 = cleanseddf10.dropDuplicates()\n",
    "cleanseddf12 = cleanseddf11.dropDuplicates(['shipment_id'])\n",
    "display(cleanseddf12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ca11c1e-5929-4454-bc01-56113d8774a2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768593534803}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logistics_shipment_df6 = logistics_shipment_df5.dropDuplicates()\n",
    "logistics_shipment_df7 = logistics_shipment_df6.dropDuplicates(['shipment_id'])\n",
    "display(logistics_shipment_df7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fb7fdd2-804c-402c-b254-dc9d736333db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Data Enrichment - Detailing of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cea6f3c8-72f7-4c15-91b8-6bd5847b93bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Add Audit Timestamp (load_dt) Source File: DF of logistics_source1 and logistics_source2\n",
    "\n",
    " - Scenario: We need to track exactly when this record was ingested into our Data Lakehouse for auditing purposes.\n",
    " - Action: Add a column load_dt using the function current_timestamp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc722af1-b3ed-484b-bb0a-e1031a702081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf13 = cleanseddf12.withColumn(\"load_dt\",current_timestamp())\n",
    "cleanseddf13.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d57f57c5-4622-4f13-b894-380733dc8d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Create Full Name (full_name) Source File: DF of logistics_source1 and logistics_source2\n",
    "\n",
    " - Scenario: The reporting dashboard requires a single field for the driver's name instead of separate columns.\n",
    " - Action: Create full_name by concatenating first_name and last_name with a space separator.\n",
    "Result: \"Rajesh\" + \" \" + \"Kumar\" -> \"Rajesh Kumar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a6dc4e6-a0ba-4a1e-a3ea-40ea3d2acee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "cleanseddf14 = cleanseddf13.withColumn(\"full_name\",concat(col(\"staff_first_name\"),lit(\" \"),col(\"staff_last_name\")))\n",
    "display(cleanseddf14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6c8317f-5081-4a5a-89f8-4802f64e1471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Define Route Segment (route_segment) Source File: DF of logistics_shipment_detail_3000.json\n",
    "\n",
    " - Scenario: The logistics team wants to analyze performance based on specific transport lanes (Source to Destination).\n",
    " - Action: Combine source_city and destination_city with a hyphen.\n",
    " - Result: \"Chennai\" + \"-\" + \"Pune\" -> \"Chennai-Pune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f06ddb4e-a806-43f4-a86a-a739a9b92c5f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768593623696}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logistics_shipment_df8 = logistics_shipment_df7.withColumn(\"route_segment\", concat(col(\"source_city\"),lit(\"->\"),col(\"destination_city\")))\n",
    "display(logistics_shipment_df8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a353c9a4-5a0e-41d3-9629-a930328fbc3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Generate Vehicle Identifier (vehicle_identifier) Source File: DF of logistics_shipment_detail_3000.json\n",
    "\n",
    " - Scenario: We need a unique tracking code that immediately tells us the vehicle type and the shipment ID.\n",
    " - Action: Combine vehicle_type and shipment_id to create a composite key.\n",
    " - Result: \"Truck\" + \"_\" + \"500001\" -> \"Truck_500001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0ff2054-d479-49b9-812f-8f9d057da24e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf15 = cleanseddf14.withColumn(\"vehicle_identifier\",concat(col('vehicle_type'),lit(\"_\"),col('shipment_id')))\n",
    "display(cleanseddf15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d4c6be1-657b-48d7-ad87-2138d4ea3994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Derive Shipment Year (shipment_year)\n",
    "\n",
    " - Scenario: Management needs an annual performance report to compare growth year-over-year.\n",
    " - Action: Extract the year component from shipment_date.-\n",
    " - Result: \"2024-04-23\" -> 2024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6ab1dbb-09cb-49d3-89dd-8e283ba89d14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logistics_shipment_df9 = logistics_shipment_df8.withColumn(\"shipment_year\",year(to_date(col('shipment_date'),'MM-dd-yyyy')))\n",
    "logistics_shipment_df9.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b236d1a8-4d76-4791-b977-1df4649ba70c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Derive Shipment Month (shipment_month)\n",
    "\n",
    " - Scenario: Analysts want to identify seasonal peaks (e.g., increased volume in December).\n",
    " - Action: Extract the month component from shipment_date.\n",
    " - Result: \"2024-04-23\" -> 4 (April)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1921f427-ad8f-4502-8369-08ab976d89e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logistics_shipment_df10 = logistics_shipment_df9.withColumn(\"shipment_month\",month(to_date(col('shipment_date'),'MM-dd-yyyy')))\n",
    "logistics_shipment_df10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28a019bf-c7c1-4362-ae05-3d4dc9d5a663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6089303672176865,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Logistics UseCase",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
